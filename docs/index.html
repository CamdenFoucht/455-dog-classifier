<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dog Classification Project</title>
    <link rel="stylesheet" href="style.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Fira+Sans:wght@400;500;600;700;900&display=swap"
        rel="stylesheet">


    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;500;600;700;800;900&display=swap"
        rel="stylesheet">
        <link rel="icon" type="image/x-icon" href="./favicon.ico">

<body>
    <header class="header">
        <div class="container">
            <div class="header-row">
                <div class="header-row-item">
                    <h1 class="header-title">Classifying Dog Breeds</h1>
                    <p class="header-p">A writeup on the problem, approach, datasets, and results classifying dogs with Pytorch.</p>
                    <p class="header-p">By Yodahe and Camden</p>
                </div>
                <div class="header-right">
                    <!-- <img class="header-img" src="https://miro.medium.com/max/640/1*cY9AOTe3vjlerL73ItnfTg.jpeg" alt=""> -->
                    <img class="header-img"
                        src="https://www.hepper.com/wp-content/uploads/2021/11/golden-retriever_Olena-Brodetska_Shutterstock.jpg"
                        alt="">
                </div>
            </div>

        </div>
    </header>

    <main class="container">
        <p class="section-text" style="margin-top: 3rem;">
            The code for building our model can be found on this <a class="link" href="https://colab.research.google.com/drive/1GomG8GxtTg6NCnP06Kl2pIZRXxcI84k9?usp=sharing#scrollTo=-gY232ZgWkAE">Google Colab Notebook</a> and the Simple UI can be found on 
            <a class="link" href="https://ml-dog-breed-classifier.herokuapp.com/">ml-dog-breed-classifier.herokuapp.com</a>. The UI repo for the heroku flask app can be found here on <a class="link" href="https://github.com/CamdenFoucht/dog-classifier">GitHub</a>.
        </p>
        <section>
            <h2 class="section-header">
                Project Video
            </h2>
            <p class="section-text">
                <video controls width="250" controls poster="">

                    <source src="https://archive.org/download/ElephantsDream/ed_hd.ogv" type="video/mp4">

                    Sorry, your browser doesn't support embedded videos.
                </video>

            </p>
        </section>

        <section>
            <h2 class="section-header">
                Problem And Motivation
            </h2>
            <p class="section-text">
                Classification has become quite popular in Computer Vision, and being able to classify objects has
                become much more feasible in recent years.
                Have you ever strolled through a dog park and seen an unusual dog but have no idea what breed that dog
                may be? With so many different dog breeds
                today it's hard to stay in the know. You may see a dog that really captures your attention with it's
                majestic coat or it's calming tempermaent, but you may
                be unsure of it's temperament.

                In our project we will be focusing on creating a Convolutional Neural Net model to be able to classify a
                dog's breed from an image. We want to attempt to solve this problem by being able to classify a dog's
                breed by just analyzing an image.


            </p>
        </section>


        <section>
            <h2 class="section-header">
                Our Approach
            </h2>
            <p class="section-text">
                Our approach is to leverage the Pytorch library by using the utility functions for creating datasets like ImageFolder, DataLoader, Image Tensors, Transforms and more. We also want to utilize their pretrained models so we can hit the floor running with our dog breed classifications. By leveraging a pretrained model we can take advantage of a model that has already been trained on over a million images and specially crafted model layers. We will spend more of our time fine tuning the model by focusing on selecting the correct parameters and preparing the data to perform the dog classification training.
            </p>
        </section>

        <section>
            <h2 class="section-header">
                Datasets & Cleaning
            </h2>
            <p class="section-text">
                The dataset we used was the <a class="link"
                    href="http://vision.stanford.edu/aditya86/ImageNetDogs/">Stanford Dogs Dataset</a> It contains
                images of 120 breeds of dogs from around the world. The dataset contains 20,580 images.
                <br /><br />
                The dataset contained a folder for each breed prepended with some hash like "n02086240-German_shepherd".
                To create our labels we looped through the directory and eliminated the prepending string and deleted
                underscores to get "German Shepherd" and appended this to a labels array. We now had an array of labels
                containg every dog breed within the dataset. Here is a preview of the first 20 labels. (To view the full labels please view the   <a class="link" href="https://colab.research.google.com/drive/1GomG8GxtTg6NCnP06Kl2pIZRXxcI84k9#scrollTo=GLaW04i26gna">Colab Notebook</a>)
                <br/>
                <code style="display: block; padding: 0.25rem; margin-top: 0.5rem; background: #eee;">['Chihuahua', 'Japanese spaniel', 'Maltese dog', 'Pekinese', 'Shih Tzu', 'Blenheim spaniel',
                    'papillon', 'toy terrier', 'Rhodesian ridgeback', 'Afghan hound', 'basset', 'beagle', 'bloodhound',
                    'bluetick', 'black and tan coonhound', 'Walker hound', 'English foxhound', 'redbone', 'borzoi',
                    'Irish wolfhound', ...]
                </code>
                <br />
                The dataset did not come with train, valid, and test folders so we had to create our own. We chose to
                use 25% of the images in the dataset as testing images, and leveraged Pytorch's random_split method to
                create a test dataset consisting of 5300 images, a valid dataset consisting of 1440 images, and train
                dataset consisting of 13840 images.

                <br /></br>
                The images contained within the dataset were not sized the same, and because we wanted to use a CNN model we needed to ensure they had the same dimensions. Pytorch's ImageFolder method 
                takes a transform argument which transforms each image in the folder. We use Pytorch's Compose utility to transform each image into a Tensor and then resized the image to 256x256 pixels.

            </p>
            <div style="display: flex;">
                <div style="margin-right: 5rem;">
                    <h3>Before (500 x 333)</h3>
                    <img src="http://vision.stanford.edu/aditya86/ImageNetDogs/images/n02106662-German_shepherd/n02106662_18405.jpg" />
                </div>
                <div>
                    <h3>After (256 x 256)</h3>
                    <img src="http://vision.stanford.edu/aditya86/ImageNetDogs/images/n02106662-German_shepherd/n02106662_18405.jpg" style="width: 256px; height: 256px; object-fit:cover; object-position: center;"/>
                </div>
            </div>
            <p class="section-text">
                We also performed random rotations on our training dataset to let our model be rotation invariant. With our Pytorch compose method, we used a random rotate on the images on the training dataset so or CNN will be fed randomly rotated images. Here is an example of a rotated image from the transformed training dataset.
            </p>
            <div style="display: flex; align-items: center;">
                <div>
                    <img src="./dog-before.png" class="resize-img"/>
                </div>
            </div>
        </section>

        <section>
            <h2 class="section-header">
                Initial Model Selection & Definition
            </h2>
            <p class="section-text">
                We wanted to used a pretrained model to aid in our classification so we needed to choose a model from one of Pytorch's available models. Unsure of which one to go with, we decided to go start with Resnet50 as it was highly acclaimed and with 50 layers and pretraining of over a million images, we thought this would be a good starting spot.

                <br/>

                Following some references we found online, we needed to create an ImageClassificationBase class which would be a super class of the pretrained model. We then created a class PretrainedResnetModel in which we overwrote the last layer in the model. We use a sequential layer where we took in the previous layer output and outputted 120 (the number of dog breeds). We then took the LogSoftMax to convert the output into a probability distribution.

            </p>
            <img style="width: 100%" src="https://miro.medium.com/max/1400/1*DBACwWr95eCibSw2EmzhVw.png" />
        </section>
        


        <section>
            <h2 class="section-header">
                Choosing Parameters
            </h2>
            <p class="section-text">
                Each time our model trains, it will be getting a batch from a dataset which we have configured to be 64 images. Which you can see below.
            </p>
            <img class="batch-img" src="./batch.png"/>

            <p class="section-text">
                We had to choose several parameters when training our model such as the learning rate, epochs, weight decay, and an optimization function. We decided to use a trial and error approach to determine what parameters we should use. We put the results of our investigation into the tables below. 
            </p>
            <table>
                <thead>
                    <th>Model</th>
                    <th>Num Epochs</th>
                    <th>Learning Rate</th>
                    <th>Weight Decay</th>
                    <th>Optimization Function</th>
                    <th>Accuracy</th>
                </thead>
                <tbody>
                    <tr>
                        <td>ResNet-50</td>
                        <td>5</td>
                        <td>0.1</td>
                        <td>0.0001</td>
                        <td>torch.optim.SGD</td>
                        <td>65.9%</td>
                    </tr>
                    <tr>
                        <td>ResNet-50</td>
                        <td>5</td>
                        <td>0.01</td>
                        <td>0.0001</td>
                        <td>torch.optim.SGD</td>
                        <td>80.77%</td>
                    </tr>
                    <tr>
                        <td>ResNet-50</td>
                        <td>5</td>
                        <td>0.001</td>
                        <td>0.0001</td>
                        <td>torch.optim.SGD</td>
                        <td>78.43%</td>
                    </tr>
                    <tr>
                        <td>ResNet-50</td>
                        <td>5</td>
                        <td>0.0001</td>
                        <td>0.0001</td>
                        <td>torch.optim.SGD</td>
                        <td>25.53%</td>
                    </tr>
                </tbody>
            </table>
            <img src="./learning-rate-graph.png" style="margin-top: 1rem; width: 100%" alt="">
            <p class="section-text">
                After our trial and error sessions, we decided to start our parameters at with a learning rate of <strong>0.01</strong>, a weight decay of <strong>0.0001</strong>, and a <strong>torch.optim.SGD</strong> optomizer. After running 15 epochs, our accuracy rate was at 88%.

            </p>
        </section>

        <section>
            <h2 class="section-header">
                Trying Other Models
            </h2>
            <p class="section-text">
               We decided to try four other models to compare our results with. We chose GoogleNet, VGG16, Wide Resnet-50, and Resnet-100 to test. After hours of training we have put the results in a table below.
            </p>

            <p class="section-text">
                We had to choose several parameters when training our model such as the learning rate, epochs, weight decay, and an optimization function. We decided to use a trial and error approach to determine what parameters we should use. We put the results of our investigation into the table below. 
            </p>
            <table>
                <thead>
                    <th>Model</th>
                    <th>Num Epochs</th>
                    <th>Accuracy</th>
                </thead>
                <tbody>
                    <tr>
                        <td>ResNet-50</td>
                        <td>10</td>
                        <td>82%</td>
                    </tr>
                    <tr>
                        <td>Wide Resnet-50</td>
                        <td>10</td>
                        <td>85.12%</td>
                    </tr>
                    <tr>
                        <td>Resnet-100</td>
                        <td>10</td>
                        <td>81.4%</td>
                    </tr>
                    <tr>
                        <td>GoogleNet</td>
                        <td>10</td>
                        <td>77.4%</td>
                    </tr>
                    <tr>
                        <td>VGG16</td>
                        <td>10</td>
                        <td>76.3%</td>
                    </tr>                
                </tbody>
            </table>
            <img src="./model-acc.png" style="width: 100%"/>
            <img src="./model-loss.png" style="width: 100%; margin-top: 2rem;"/>
            <p class="section-text">
                After these tests, our best performing model was the wide resnet-50 pretrained model with a validation accuracy of 85% after just 10 epochs. We can see after the first epoch it was already in the 70's for it's accuracy percentage. Meanwhile the VGG16 model and GoogleNet models started much lower around 30-40% but quickly rose to the 70's after 8 epochs. We decided to stick with the <strong>wide resnet-50</strong> model for further training. 
            </p>
        </section>

        <section>
            <h2 class="section-header">
                Results
            </h2>
            <p class="section-text">
                Lorem ipsum dolor sit amet consectetur adipisicing elit. Accusamus vel omnis sint laborum officiis
                consequatur neque atque soluta, ducimus rerum, itaque aliquam earum corrupti, explicabo eveniet ex
                deserunt sunt quaerat!
            </p>
        </section>

        <section>
            <h2 class="section-header">
                Simple Web Deployment
            </h2>
            <p class="section-text">
                While we had our model on Google colab, we wanted users to be able to interact with our model and load their own images to test. We decided to use Flask to deploy a simple web server on Heroku to host our model logic. We built a very simple UI that allows a user to provide a link to an image or upload their own image. The image will then be sent to our model and it will make a prediction.
                <br/><br/>
                We had to figure out how to host our model on Heroku without requiring a user to download large files when loading a web app. We decided to train a web specific model using densenet which was only 20mb in syze compared to 260 mb from the resnet model. We then used the pickle library to serialize and compress our model that heroku could then load. We also ran into memory errors trying this due to Pytorch's library being too large. We solved this by finding a cpu specific Pytorch library which reduce the server size by several hundred mb. With these fixes we were able to deploy a very simple web version of our model, which you can see below. 
            </p>

            <img src="./web-example.png" class="web-example-img" alt="Example of the heroku web app"/>
        </section>
        <section>
            <h2 class="section-header">
                Dicussion
            </h2>
            <p class="section-text">
                <h3>Problems Encountered</h3>
                <div class="pad-left">
                    <p class="section-text">
                        When we first started we loaded a pretrained model and didn't realize we needed to replace the last layer in the model. This led us to looking through StackOverflow until we realized we needed to overwrite the last layer.
                    </p>
                    <p class="section-text">
                        We also ran into ours during our model building/training process because our images weren't the same size. We had to research how we could resize the entire dataset to get our CNN to function, and we had to discover the Pytorch transform resize() method to do this.
                    </p>
                    <p class="section-text">
                        Memory errors. We often ran into cuda memory errors due to bad parameters or batches that were too big. This led to us refining our parameters often.
                    </p>
                </div>
                <h3>Next Steps</h3>
                <div class="pad-left">
                    <p class="section-text">
                        Our next steps would be to expand the UI so the images users provided would also train the model. We could create functionality for a user to provide what the real label of the dog was if they know, or provide an input for saying if the prediction was correct. Our web version also uses a slightly lower accuracy model due to a smaller file size on the Heroku free tier. We would like to use our stronger models in a webapp so we can have higher accuracies.
                    </p>
                </div>   
                <h3>Takeaways</h3>
                <div class="pad-left">
                    <p class="section-text">
                        Overall, this project was full of learning. We got experience cleaning data with python, creating datasets, discovering new models, and learning about how to finetune parameters. We learned about the utilties of the Pytorch library and how to quickly build a ML model using their pretrained models. We also gained experience with Flask and deployments, and how to compress fully built models into a pickle format for deployment.
                    </p>
                </div>                
            </p>
        </section>
        <section>
            <h2 class="section-header">
                References
            </h2>
                <a class="link d-block my-1" href="#">Some Link Here</a>  
                <a class="link d-block my-1" href="#">Some Link Here</a>  
                <a class="link d-block my-1" href="#">Some Link Here</a>  
                <a class="link d-block my-1" href="#">Some Link Here</a>  

        </p>
        </section>
    </main>
</body>

</html>